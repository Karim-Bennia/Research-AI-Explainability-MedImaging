# AI Explainability in Medical Imaging
![Research Context](assets/context_image.png)
## Project Overview

**Core Idea**  
In the field of medical imaging, AI models are vital in assisting healthcare professionals with diagnosing diseases and making treatment decisions. However, the opaque nature of some AI algorithms can raise concerns regarding their reliability and trustworthiness. Explainability in AI refers to the techniques that allow us to understand and interpret the decisions made by these models. It enhances transparency and builds trust among doctors and patients in the technology.

## Methods

We employed several state-of-the-art explainability methods to interpret AI models in the context of medical imaging. The methods included:

- **Guided Backpropagation**
- **Integrated Gradients**
- **LIME (Local Interpretable Model-agnostic Explanations)**
- **SHAP (SHapley Additive exPlanations)**
- **XRAI (eXtended Relative Attributions via Integrals)**
- **Counterfactual Explanations**

our team has made substantial progress in implementing and evaluating different XAI techniques. This project not only enhanced our understanding of AI interpretability but also set a benchmark for future research in AI explainability in medical imaging. We believe that the insights gained from this project can help in the design of more transparent AI systems, fostering a deeper trust and broader acceptance of AI technologies in medical diagnostics.

